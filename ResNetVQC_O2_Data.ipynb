{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence annoying TF crap printed on the screen\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"  # or even \"-1\"  \n",
    "\n",
    "# TensorFlow,  tf.keras and matplotlib stuff\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper libraries\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "\n",
    "# VQCLayer\n",
    "from VQCLayer import makeVQCLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_runtime = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# To find out which devices your operations and tensors are assigned to\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.list_physical_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path, get_SNR = False):\n",
    "\n",
    "    imgs=list(Path(path).glob('./*.png'))\n",
    "    images=list()\n",
    "    if get_SNR: SNR = list()\n",
    "    \n",
    "    # Loop over the images  \n",
    "    for filename in imgs:\n",
    "\n",
    "        #get the images already normalized\n",
    "        img_data = asarray(PIL.Image.open(str(filename)))/255.0\n",
    "\n",
    "        images.append(img_data)\n",
    "\n",
    "        if get_SNR:\n",
    "            #get the SNR from the file name \n",
    "            #format (Shifted_H1%s%.f_V1%s%.f_%.3f_%.1f.png)%(p/m, |shift_H1|, p/m, |shift_V1|, t_trigger, SNR)\n",
    "            head, tail = os.path.split(filename)\n",
    "            SNR.append(float(re.findall(\"\\d+\\.\\d+\", tail)[-1]))\n",
    "\n",
    "    if get_SNR: return asarray(images), asarray(SNR)\n",
    "    else: return asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it starts   \n",
    "\n",
    "# These are the names of the classes\n",
    "# We only have two classes: 0=noise, 1=GW event\n",
    "class_names = ['0', '1']\n",
    "\n",
    "#minimum value of the probability to be considered gravitational wave\n",
    "disc = 0.9\n",
    "\n",
    "#number of epochs to train NN\n",
    "epoch_num = 6\n",
    "#epoch_num = 12 : original\n",
    "\n",
    "#Show the training curves for the validation images\n",
    "no_train_curves = False\n",
    "\n",
    "# Set the root path of the data\n",
    "datapath = 'C:/Users/koich/Desktop/temporary/ReducedSamplesQuantum/'\n",
    "test_to_exp =  1/18.0\n",
    "\n",
    "# Set the test paths\n",
    "test_path_GWs=datapath+'Validation_General_Injected_together/'\n",
    "test_path_noise=datapath+'Validation_CoincidenceBackground_together/'\n",
    "\n",
    "# Set the training paths\n",
    "train_path_GWs=datapath+'General_Injected_together/'\n",
    "train_path_noise=datapath+'CoincidenceBackground_together/'\n",
    "\n",
    "# Set the O2 data path\n",
    "O2_data_path = datapath+'O2_Data_together/'\n",
    "\n",
    "# Set output path\n",
    "out_path = 'C:/Users/koich/Desktop/temporary/ReducedSamplesQuantum/Out/temp/'\n",
    "\n",
    "# load the base model\n",
    "base_model_path = 'C:/Users/koich/Desktop/temporary/ReducedSamplesQuantum/Out/out202111251108/ResNet_v1.h5'\n",
    "\n",
    "# constructed model\n",
    "model_path = out_path + 'model.h5'\n",
    "\n",
    "# VQC settings\n",
    "numQubit = 4\n",
    "depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GW test images\n",
    "print('Now reading the GW test images...')\n",
    "test_images_GWs = get_images(test_path_GWs)\n",
    "test_labels_GWs = np.repeat(1,len(test_images_GWs))\n",
    "print(test_images_GWs.shape)\n",
    "\n",
    "# Read the noise test images\n",
    "print('Now reading the noise test images...')\n",
    "test_images_noise = get_images(test_path_noise)\n",
    "test_labels_noise = np.repeat(0,len(test_images_noise))\n",
    "print(test_images_noise.shape)\n",
    "\n",
    "#Merge the two\n",
    "test_images = np.concatenate((test_images_GWs,test_images_noise))\n",
    "test_labels = np.concatenate((test_labels_GWs,test_labels_noise))\n",
    "del test_images_GWs, test_images_noise, test_labels_GWs, test_labels_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel= tf.keras.models.load_model(base_model_path)\n",
    "baseModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GW training images\n",
    "print('Now reading the GWs train images...')\n",
    "train_images_GWs=get_images(train_path_GWs)\n",
    "train_labels_GWs=np.repeat(1,len(train_images_GWs))\n",
    "print(train_images_GWs.shape)\n",
    "\n",
    "# Read the noise training images\n",
    "print('Now reading the noise train images...')\n",
    "train_images_noise=get_images(train_path_noise)\n",
    "train_labels_noise=np.repeat(0,len(train_images_noise))\n",
    "print(train_images_noise.shape)\n",
    "\n",
    "#Gather all the train images\n",
    "train_images=np.concatenate((train_images_GWs,train_images_noise))\n",
    "\n",
    "#Gather all the train labels\n",
    "train_labels=np.concatenate((train_labels_GWs,train_labels_noise))\n",
    "\n",
    "#liberate unused memory\n",
    "del train_images_GWs, train_images_noise, train_labels_GWs, train_labels_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseOut = baseModel.layers[-2].output \n",
    "inQ = tf.keras.layers.Dense(numQubit, activation='sigmoid')(baseOut)\n",
    "outQ = makeVQCLayer(numQubit, depth)(inQ)\n",
    "\n",
    "# layer to convert Z to 0.5*(Z+1)\n",
    "outLayer = tf.keras.layers.Dense(1,\n",
    "                                 kernel_initializer=tf.keras.initializers.Constant(value = 0.5),\n",
    "                                 bias_initializer=tf.keras.initializers.Constant(value = 0.5))\n",
    "out = outLayer(outQ)\n",
    "                                 \n",
    "model = tf.keras.models.Model(inputs=baseModel.input, outputs=out)\n",
    "\n",
    "numLayers = len(model.layers)\n",
    "for i in list(range(numLayers - 3)) + [numLayers - 1]:\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Now starting the training!')\n",
    "#fit the model\n",
    "history = model.fit(train_images, train_labels, epochs=epoch_num, batch_size = 32, verbose=2, validation_data = (test_images, test_labels)) \n",
    "\n",
    "print('Now saving model...')\n",
    "model.save(model_path)\n",
    "\n",
    "#liberate memory\n",
    "del train_images, train_labels, baseModel, out, output\n",
    "\n",
    "#generate the plots with the training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.figure(6,figsize = (15,10))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "plt.plot(np.arange(epoch_num)+1, history.history['accuracy'], label = 'Training', linewidth = 2)\n",
    "plt.plot(np.arange(epoch_num)+1, history.history['val_accuracy'], label = 'Validation', linewidth = 2)\n",
    "plt.ylabel(r'Accuracy')\n",
    "plt.xlabel(r'Epoch')\n",
    "plt.xlim(1,epoch_num)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(out_path + \"Accuracy.png\", bbox_inches=\"tight\")\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(7,figsize = (15,10))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "plt.plot(np.arange(epoch_num) + 1, history.history['loss'], label = 'Training', linewidth = 2)\n",
    "plt.plot(np.arange(epoch_num) + 1, history.history['val_loss'], label = 'Validation', linewidth = 2)\n",
    "plt.ylabel(r'Loss')\n",
    "plt.xlabel(r'Epoch')\n",
    "plt.xlim(1,epoch_num)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(out_path + \"Loss.png\", bbox_inches=\"tight\")\n",
    "\n",
    "#save training history\n",
    "np.savetxt(out_path + \"history.txt\", np.transpose([history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy']]))\n",
    "del history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test validation\n",
    "test_loss, test_acc = model.evaluate(test_images,test_labels,verbose=0)\n",
    "\n",
    "print(\"test_loss = \", test_loss, \"        test_acc = \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities and save the training\n",
    "predictions = asarray(model.predict(test_images))[:,0]\n",
    "\n",
    "#Assign the classes\n",
    "real_vals = asarray(test_labels)\n",
    "del test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the ROC curve of the NN\n",
    "tROC = np.linspace(0,1,101)\n",
    "false_positive_frac_t = np.array([])\n",
    "true_positive_frac_t = np.array([])\n",
    "\n",
    "for t_ROC_c in tROC:\n",
    "\n",
    "    #Assign predicted class\n",
    "    predclass_t = np.where(predictions>t_ROC_c, 1, 0)\n",
    "\n",
    "    #truth table\n",
    "    true_positives_t = np.sum(np.where(np.logical_and(predclass_t == 1, real_vals == predclass_t), 1, 0))\n",
    "    false_positives_t = np.sum(np.where(np.logical_and(predclass_t == 1, real_vals != predclass_t), 1, 0))\n",
    "    true_negatives_t = np.sum(np.where(np.logical_and(predclass_t == 0, real_vals == predclass_t), 1, 0))\n",
    "    false_negatives_t = np.sum(np.where(np.logical_and(predclass_t == 0, real_vals != predclass_t), 1, 0))\n",
    "\n",
    "    #accuracy of the neural network\n",
    "    false_positive_frac_t = np.append(false_positive_frac_t, false_positives_t/(false_positives_t + true_negatives_t))\n",
    "    true_positive_frac_t = np.append(true_positive_frac_t, true_positives_t/(false_negatives_t + true_positives_t))\n",
    "\n",
    "#Plot the ROC curve\n",
    "plt.figure(1,figsize = (15,10))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "plt.plot(100*false_positive_frac_t, 100*true_positive_frac_t, linewidth = 2)\n",
    "plt.xlabel(r'False Positive Rate (%)')\n",
    "plt.ylabel(r'True Positive Rate (%)')\n",
    "plt.grid()\n",
    "plt.savefig(out_path + \"ROC.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign predicted class\n",
    "predclass = np.where(predictions>disc, 1, 0)\n",
    "\n",
    "#truth table\n",
    "true_positives = np.sum(np.where(np.logical_and(predclass == 1, real_vals == predclass), 1, 0))\t\n",
    "false_positives = np.sum(np.where(np.logical_and(predclass == 1, real_vals != predclass), 1, 0))\t\n",
    "true_negatives = np.sum(np.where(np.logical_and(predclass == 0, real_vals == predclass), 1, 0))\t\n",
    "false_negatives = np.sum(np.where(np.logical_and(predclass == 0, real_vals != predclass), 1, 0))\n",
    "print(\"\\nTrue Positive = %.f      False Positive = %.f\"%(true_positives,false_positives))\n",
    "print(\"True Negative = %.f      False Negative = %.f\"%(true_negatives,false_negatives))\n",
    "\n",
    "#accuracy of the neural network\n",
    "false_positive_frac = false_positives/(false_positives + true_negatives)\n",
    "false_negative_frac = false_negatives/(false_negatives + true_positives)\n",
    "\n",
    "print(\"\\nfalse_positive_frac = %.4f    false_negative_frac = %.4f\\n\"%(false_positive_frac, false_negative_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
